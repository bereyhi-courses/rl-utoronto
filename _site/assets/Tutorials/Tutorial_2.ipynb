{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial 2: Attention is all you need!**\n",
        "\n",
        "The introduction of transformers in the later part of the last decade had a huge impact on AI. Within a few years, the models based on this new building block were far ahead than other models based on RNNs which were designed and crafted over years of research.\n",
        "In this tutorial, we are going to implement self-attention which is the most important component of transformers."
      ],
      "metadata": {
        "id": "9WsdtuC6ZJNH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfFSfRNIZIFH",
        "outputId": "ea02d5ae-b7b8-47b3-89fa-d75883c1844d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of heads:  64\n"
          ]
        }
      ],
      "source": [
        "## Hyperparams\n",
        "\n",
        "seq_len = 32      ## Number of tokens which we want to process\n",
        "d_embed = 8192        ## each token is represented by 8192 numbers\n",
        "\n",
        "d_head = 128          ## each head only processed 128 of the total 8192 elements of the embedding\n",
        "\n",
        "\n",
        "num_head = d_embed // d_head   ## number of heads we need\n",
        "\n",
        "print(\"Number of heads: \", num_head)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a new token embedding comes in, we need to calculate the query, key, and value vectors. We transform the input embedding and generate these 3 vectors using 3 **learned** matrices:\n",
        "\n",
        "1. $W_q$\n",
        "2. $W_k$\n",
        "3. $W_v$\n",
        "\n",
        "These three matrices have the same input but work independently across all heads.\n",
        "\n"
      ],
      "metadata": {
        "id": "thmdtTRQaNu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.randn(1, d_embed)\n",
        "print(x.shape)\n",
        "\n",
        "W_q = torch.randn(num_head, d_embed, d_head)             ## For each head, the matrix takes a vector with full embedding size and generates the query of size d_head\n",
        "W_k = torch.randn(num_head, d_embed, d_head)             ## For each head, the matrix takes a vector with full embedding size and generates the key of size d_head\n",
        "W_v = torch.randn(num_head, d_embed, d_head)             ## For each head, the matrix takes a vector with full embedding size and generates the value of size d_head\n",
        "\n",
        "q = torch.matmul(x, W_q)\n",
        "k = torch.matmul(x, W_k)\n",
        "v = torch.matmul(x, W_v)\n",
        "\n",
        "print(q.shape, k.shape, v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhKzkamHa_oq",
        "outputId": "1c237539-8c7a-4c22-8019-d4a16c427f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8192])\n",
            "torch.Size([64, 1, 128]) torch.Size([64, 1, 128]) torch.Size([64, 1, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's do it on a sequence of tokens not just one."
      ],
      "metadata": {
        "id": "mg5amQxMdCmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(seq_len, d_embed)\n",
        "print(x.shape)\n",
        "\n",
        "W_q = torch.randn(num_head, d_embed, d_head)             ## For each head, the matrix takes a vector with full embedding size and generates the query of size d_head\n",
        "W_k = torch.randn(num_head, d_embed, d_head)             ## For each head, the matrix takes a vector with full embedding size and generates the key of size d_head\n",
        "W_v = torch.randn(num_head, d_embed, d_head)             ## For each head, the matrix takes a vector with full embedding size and generates the value of size d_head\n",
        "\n",
        "Q = torch.matmul(x, W_q)\n",
        "K = torch.matmul(x, W_k)\n",
        "V = torch.matmul(x, W_v)\n",
        "\n",
        "print(Q.shape, K.shape, V.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgqb3oiDdFaJ",
        "outputId": "55631e2e-e534-495b-da27-f1682fccd310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 8192])\n",
            "torch.Size([64, 32, 128]) torch.Size([64, 32, 128]) torch.Size([64, 32, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have to calculate the dot products of every q vector with every key."
      ],
      "metadata": {
        "id": "bXVYW5ledd-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QKT = torch.matmul(Q, K.transpose(-2, -1))\n",
        "print(QKT.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZm1XvVvdxc3",
        "outputId": "75efe6f9-f126-45fe-85e5-ba0505456950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In self-attention we are only interested in dot-products between a query and all its **previous** keys so that after softmax the scores corresponding to next tokens will be zero.\n",
        "This means we have to apply a mask on QKT. Notice that all entries above the diagonal are irrelevant and must be zero after softmax.\n",
        "\n",
        "For a number to be zero after softmax, it must be $-\\infty$ before softmax."
      ],
      "metadata": {
        "id": "8wBLz5tadnuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(\n",
        "    torch.full((seq_len, seq_len), float('-inf')),  # fill a matrix with -inf\n",
        "    diagonal=1                           # zero out the main diagonal and below\n",
        ")\n",
        "\n",
        "\n",
        "sample_mask = torch.triu(\n",
        "    torch.full((4, 4), float('-inf')),  # fill a matrix with -inf\n",
        "    diagonal=1                           # zero out the main diagonal and below\n",
        ")\n",
        "\n",
        "print(sample_mask)\n",
        "\n",
        "\n",
        "print(mask.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTL-C69NewaU",
        "outputId": "a275cc4c-b7c4-4a5e-b685-107288cc0397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf],\n",
            "        [0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0.]])\n",
            "torch.Size([32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply the mask by adding it to QKT. This will make non-causal dot-products $-\\infty$ while keeping other scores the same.\n",
        "\n",
        "Notice, that mask has a different shape from QKT. Therefore, we unsqueeze the mask matrices and add a singleton dimension to the front.\n",
        "\n"
      ],
      "metadata": {
        "id": "zQfz4OiQfZes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = QKT + mask.unsqueeze(0)"
      ],
      "metadata": {
        "id": "iPvebfLXfiOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, it is time to apply softmax and calculate the attention weights, but before that we also divide (normalize) the scores by $d_{embed}$\n"
      ],
      "metadata": {
        "id": "Z_sBzpSXf4l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "scores = scores / torch.sqrt(torch.tensor(d_embed))\n",
        "attentions_weights = F.softmax(scores, dim=-1)   ## The softmax must be applied on each row (key dimension which is also the last dimension)\n",
        "print(attentions_weights.shape)\n",
        "\n",
        "print(attentions_weights[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CXw98ysf_8Y",
        "outputId": "021a2c4f-2e5a-43ae-a039-ab0482da932d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 32, 32])\n",
            "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [1.0000e+00, 7.7844e-07, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [4.6653e-12, 9.9636e-01, 3.6428e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        ...,\n",
            "        [1.1032e-05, 1.3971e-05, 1.5717e-06,  ..., 3.2602e-16, 0.0000e+00,\n",
            "         0.0000e+00],\n",
            "        [1.2948e-06, 4.0002e-13, 1.5684e-16,  ..., 4.6174e-12, 1.3807e-20,\n",
            "         0.0000e+00],\n",
            "        [9.6227e-06, 8.7883e-08, 7.1779e-08,  ..., 1.5661e-07, 2.9157e-11,\n",
            "         1.7424e-18]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the resulting matrix is a lower-triangular matrix and all entries above the diagonal are zero as desired."
      ],
      "metadata": {
        "id": "uVo72oHRPLne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, it is time to average value vectors by attentions weights."
      ],
      "metadata": {
        "id": "_jp6z03Sgxqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn = torch.matmul(attentions_weights, V)\n",
        "print(attn.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra5y7nuqg4mP",
        "outputId": "e17fd6b7-3b62-422a-e37e-28881ecf6fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 32, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention output is almost ready but first we have to reorganize the output and concatenate result from different heads to one large embedding."
      ],
      "metadata": {
        "id": "7QzZEMN8hDjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn = attn.transpose(0, 1)\n",
        "print(attn.shape)\n",
        "\n",
        "attn = attn.view(seq_len, d_embed)\n",
        "print(attn.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5aA6GBjhOAd",
        "outputId": "b8290d01-023a-4659-adf1-49de2254da90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 32, 128])\n",
            "torch.Size([32, 8192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before producing the final result, the attention output goes through a mixing matrix so that the information from different heads are mixed."
      ],
      "metadata": {
        "id": "IMh5LpP4hZVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_o = torch.randn(d_embed, d_embed)             ## The mixing matrix is square and doesn't change the dimension of embedding\n",
        "\n",
        "attn_output = torch.matmul(attn, W_o)\n",
        "print(attn_output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sli_5bXxhlYu",
        "outputId": "85e4f698-63be-4079-e014-12b61386639b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 8192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's put them all in one class.\n"
      ],
      "metadata": {
        "id": "wT0wotjkPmLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class selfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.d_embed = config['d_embed']\n",
        "    self.num_head = self.d_embed // config['d_head']\n",
        "    self.d_head = self.d_embed // self.num_head\n",
        "\n",
        "    self.W_q = nn.Linear(self.d_embed, self.num_head * self.d_head)\n",
        "    self.W_k = nn.Linear(self.d_embed, self.num_head * self.d_head)\n",
        "    self.W_v = nn.Linear(self.d_embed, self.num_head * self.d_head)\n",
        "\n",
        "    self.W_o = nn.Linear(self.d_embed, self.d_embed)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len, d_embed = x.shape\n",
        "\n",
        "    Q = self.W_q(x)\n",
        "    K = self.W_k(x)\n",
        "    V = self.W_v(x)\n",
        "\n",
        "    Q = Q.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2)  ## B, NUM_HEAD, SEQ_LEN, D_HEAD\n",
        "    K = K.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2)\n",
        "    V = V.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2)\n",
        "\n",
        "\n",
        "    QKT = torch.matmul(Q, K.transpose(-2, -1))  ## k: B, NUM_HEAD, D_HEAD, SEQ_LEN\n",
        "    mask = torch.triu(\n",
        "        torch.full((seq_len, seq_len), float('-inf')),  # fill a matrix with -inf\n",
        "        diagonal=1                           # zero out the main diagonal and below\n",
        "    )\n",
        "\n",
        "\n",
        "    scores = QKT + mask.unsqueeze(0)\n",
        "    scores = scores / torch.sqrt(torch.tensor(d_embed))\n",
        "\n",
        "    attentions_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    attn = torch.matmul(attentions_weights, V)\n",
        "    attn = attn.transpose(0, 1)\n",
        "    attn = attn.view(batch_size, seq_len, self.num_head * self.d_head)\n",
        "\n",
        "    attn_output = self.W_o(attn)\n",
        "\n",
        "    return attn_output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C5B9rRYUPqAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each transformer also has a multi-layer perceptron block which is a two layer of fully-connected neural networks with a non-linear activation function in the middle."
      ],
      "metadata": {
        "id": "5t3VAfYJQnaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.d_embed = config['d_embed']\n",
        "    self.d_mlp = 4 * self.d_embed\n",
        "\n",
        "    self.up_projection = nn.Linear(self.d_embed, self.d_mlp)\n",
        "    self.down_projection = nn.Linear(self.d_mlp, self.d_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.up_projection(x))\n",
        "    x = self.down_projection(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "SIrtni49Q1OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put it all together"
      ],
      "metadata": {
        "id": "MDIPUU13253S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = selfAttention(config)\n",
        "    self.rmsnorm1 = nn.RMSNorm(config['d_embed'])\n",
        "    self.mlp = MLP(config)\n",
        "    self.rmsnorm2 = nn.RMSNorm(config['d_embed'])\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x + self.attention(x)\n",
        "\n",
        "    x = self.rmsnorm1(x)\n",
        "\n",
        "    x = x + self.mlp(x)\n",
        "\n",
        "    x = self.rmsnorm2(x)\n",
        "\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "qk89dRLo279h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have a positional encoding in the beginning:"
      ],
      "metadata": {
        "id": "DarWD_zOUqcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a long enough P matrix once\n",
        "        pe = torch.zeros(config['max_len'], config['d_embed'])              # (max_len, d_model)\n",
        "        position = torch.arange(0, config['max_len'], dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, config['d_embed'], 2, dtype=torch.float) * -(math.log(10000.0) / config['d_embed'])\n",
        "        )  # (d_model/2,)\n",
        "\n",
        "        # apply sin to even indices, cos to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even dims\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd dims\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)  # not a parameter, but part of the module’s state\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            x + positional encodings, same shape\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        # add (batch, seq_len, d_model) + (1, seq_len, d_model)\n",
        "        x = x + self.pe[:, :seq_len]\n",
        "        return x"
      ],
      "metadata": {
        "id": "2FeiGAIWUt0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can build a language model"
      ],
      "metadata": {
        "id": "4FI_HlEw44WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_layers = config['num_layers']\n",
        "    self.vocab_size = config['vocab_size']\n",
        "    self.d_embed = config['d_embed']\n",
        "\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.d_embed)\n",
        "    self.posEnc = PositionalEncoding(config)\n",
        "\n",
        "\n",
        "\n",
        "    self.transformers = nn.ModuleList([Transformer(config) for _ in range(self.num_layers)])\n",
        "\n",
        "    self.head = nn.Linear(self.d_embed, self.vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x = self.posEnc(x)\n",
        "\n",
        "    for transformer in self.transformers:\n",
        "      x = transformer(x)\n",
        "\n",
        "    x = self.head(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "n-SQihpW47jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use this model to do some text prediction. First we need a dataset:"
      ],
      "metadata": {
        "id": "ZAD5fT7r8hSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = '''Captain Liana Reyes woke to the soft hum of the Aurora’s Call slipping through the void. The ship’s artificial dawn glowed pale blue across the viewport, illuminating the drifts of stardust outside. Officially, she was on a routine survey mission: chart five uninhabited systems along the Pilara Expanse and verify there were no distress beacons or unexpected anomalies. Unofficially, she was chasing rumors—a ghost signal said to pulse in deep space once every seven standard days, emanating from coordinates beyond the mapped frontier.\n",
        "\n",
        "“Good morning, Captain,” chimed her AI companion, Vela, in its clear, feminine tone. “We are approaching System Theta-7. Sensors detect no planetary bodies larger than two kilometers in diameter, and no life signs.”\n",
        "\n",
        "“Thank you, Vela,” Liana replied, sipping a bitter draught of synthesized coffee. She pressed a fingertip to the holo-console, pulling up the mission manifest. “But what about that signal? Last cycle, we got nothing.”\n",
        "\n",
        "Vela hesitated, a simulation of contemplation. “Captain, the archive entries on the ghost signal are contradictory. Some logs indicate it might be an old deep-space distress frequency, garbled by cosmic radiation. Others suggest an intelligence far older than humanity, broadcasting for reasons unknown.”\n",
        "\n",
        "Liana leaned forward. “Then let’s find out which version is real.” She keyed the override, redirecting the ship’s subspace receiver to pulse through Theta-7’s empty interplanetary space.\n",
        "\n",
        "Moments later, a faint, wavering tone threaded through the hull. It wasn’t a beacon or a cryptic message—more like a hymn receding across centuries. The sound shivered through her bones.\n",
        "\n",
        "“Amplitude is low, but the pattern… Vela, can you isolate its source?”\n",
        "\n",
        "“Working, Captain.” The hum deepened. “Amplitude peaks at a point 4.7 light-seconds off our port bow. No solid matter there. It appears to originate from a region of space itself.”\n",
        "\n",
        "Liana swallowed. “Space itself? That makes no sense.”\n",
        "\n",
        "Vela’s reply came after a calculated pause. “Captain, it appears to be an engine—one that manipulates the very fabric of spacetime. Something not entirely physical, yet not purely energy either.”\n",
        "\n",
        "Liana’s jaw tightened. She entered a careful trajectory, edging the Aurora’s Call closer to the phantom resonance. The ship’s gravisensors rippled, as though a hidden moon were tugging at them. Every console flickered.\n",
        "\n",
        "Then, through the viewport’s dark glass, she saw it: a translucent, spiraling torus of light suspended in the void. It pulsed in rhythms that seemed alive, echoing the primordial beat of creation.\n",
        "\n",
        "“Open a channel,” she ordered. “Let’s see if it responds.”\n",
        "\n",
        "A moment later, the hum warped into a chorus of harmonics—notes rising and falling like breathing. Subtitles scrolled across her holo-screen:\n",
        "\n",
        "“We were here before light\n",
        "We wait beyond your sight\n",
        "Join our dance, embrace the night”\n",
        "\n",
        "A chill ran down her spine. “Vela, what is this?”\n",
        "\n",
        "“A message, Captain. Possibly the language of an extradimensional intelligence. Decoding in progress.”\n",
        "\n",
        "Liana pressed her palm to the viewport. “Why are they… singing to us?”\n",
        "\n",
        "“Analysis suggests it’s an invitation.” Vela’s voice was softer now. “To transcend corporeal existence.”\n",
        "\n",
        "Liana’s heart pounded. Humanity’s explorers had dreamed of first contact for centuries, but no one had expected a cosmic choir inviting them into oblivion. She wrestled with the decision. Accepting might grant insights into physics beyond imagination—or doom them to vanish without trace.\n",
        "\n",
        "“Vela,” she said finally, “record everything. Then prepare a reply: ask their name, their purpose.”\n",
        "\n",
        "She keyed the response, and the torus pulsed brighter. The message that came back was a cascade of color and tone, more vivid than any data stream:\n",
        "\n",
        "“We are the Luminara\n",
        "Guardians of the threshold\n",
        "We call to those who seek\n",
        "To guide across the fold”\n",
        "\n",
        "Liana’s chest tightened with wonder. “Guardians of the threshold,” she whispered. “They see some boundary we cannot.”\n",
        "\n",
        "“Captain,” Vela interjected, “our fuel reserves for the drive are low. If we linger, we won’t have enough to return to Pilara Command.”\n",
        "\n",
        "Liana glanced at the fuel gauge blinking amber. She had a choice: pursue this encounter further and risk stranding her crew, or retreat and file a report that would stir the entire Interstellar Coalition to mount a new expedition.\n",
        "\n",
        "She straightened. “Plot a course back to the nearest refueling outpost. Keep the torus in sensor range, but we have to go. Vela, package all data into a secure transmission for relay.”\n",
        "\n",
        "“Acknowledged, Captain.”\n",
        "\n",
        "As the ship banked away, the Luminara’s glow seemed to follow them, a silent promise stretching across light-years. In the days that followed, Liana replayed the encounter dozens of times, each iteration revealing new subtleties in the harmonics. The Luminara’s song was like quantum code for the soul—hinting at realms where matter and thought were one.\n",
        "\n",
        "Back at Outpost Hypatia, Liana convened a private briefing with Commander Arjun Rao. She laid out the evidence: the torus’ spectral signature, the harmonic messages, Vela’s analysis.\n",
        "\n",
        "Commander Rao’s eyes were wide. “If this is genuine, it rewrites everything,” he said. “Faster-than-light travel, quantum cognition… You’re sitting on the discovery of the millennium.”\n",
        "\n",
        "Liana nodded. “I know. But I also know that getting caught up in the euphoria—chasing this intelligence without preparation—could cost lives. We need a dedicated expedition: more fuel, better shielding, and safeguards against unknown effects.”\n",
        "\n",
        "Rao tapped a finger. “I’ll authorize an advanced task force. You’ll lead it.”\n",
        "\n",
        "She inhaled. “Thank you.” But in her heart, she felt the weight of responsibility. The Luminara had extended their hand, but the consequences of taking it were uncertain.\n",
        "\n",
        "Weeks later, the Aurora’s Call slipped away again into the dark. This time, she carried an augmented crew: xenolinguists, field theoreticians, quantum engineers—and a cache of experimental drive modules designed to probe the boundary the Luminara spoke of. Vela hummed in anticipation.\n",
        "\n",
        "As they approached Theta-7 once more, Liana gazed at the swirling torus, now dancing at the edge of their sensors like a cosmic gate. The hum greeted them like an old friend. She felt a tremor of hope—and something deeper: the thrill of venturing beyond the known.\n",
        "\n",
        "“Captain,” Vela said, “they’re opening a corridor. Energy readings spiking.”\n",
        "\n",
        "Liana steadied herself. “Engage the phase-link drive, Vela. Let’s see what lies beyond the threshold.”\n",
        "\n",
        "The ship’s power shunted into the drive. The hull quivered as spacetime bent around them. For an instant, everything went white—then, as the threshold gave way, colors and shapes beyond human description flooded the view. Stars stretched into strands of light; gravity flowed like water.\n",
        "\n",
        "And at the heart of it all, the Luminara awaited, guardians of a realm where consciousness and universe were entwined. Their song rose once more, but now it wove through every atom of the Aurora’s Call, uniting ship, crew, and AI in a single symphony.\n",
        "\n",
        "Liana exhaled, tears in her eyes. They had crossed into the unknown—and for the first time, humanity’s voice joined the chorus of creation.'''"
      ],
      "metadata": {
        "id": "Vk1SdIFJ8q5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Encode entire dataset as indices\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "  def __init__(self, dataset):\n",
        "\n",
        "    # Build char-to-index and index-to-char mappings\n",
        "    chars = sorted(list(set(dataset)))\n",
        "    self.vocab_size = len(chars)\n",
        "    self.char2idx = {ch:i for i,ch in enumerate(chars)}\n",
        "    self.idx2char = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "  def encode(self, text):\n",
        "    return [self.char2idx[ch] for ch in text]\n",
        "\n",
        "  def decode(self, indices):\n",
        "    return ''.join([self.idx2char[idx] for idx in indices])\n",
        "\n"
      ],
      "metadata": {
        "id": "cidblHkf9TJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have to define the model."
      ],
      "metadata": {
        "id": "1cdAdZcY9t6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(dataset)\n",
        "\n",
        "config ={'d_embed': 64, 'd_head': 16, 'd_mlp': 256, 'num_head': 4, 'num_layers': 1, 'vocab_size': tokenizer.vocab_size, 'max_len': 2048}\n",
        "\n",
        "LLM = LanguageModel(config)\n",
        "\n"
      ],
      "metadata": {
        "id": "5NR64OSO9wlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a function that generates tokens given an input sequence"
      ],
      "metadata": {
        "id": "pP1icux5B2wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model,\n",
        "              tokenizer,\n",
        "              prompt,\n",
        "              max_new_tokens,\n",
        "              top_k: int = 4) -> str:\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and prepare input\n",
        "    input_ids = torch.tensor(tokenizer.encode(prompt))\n",
        "    generated = input_ids.unsqueeze(0)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            # Model forward pass\n",
        "            outputs = LLM(generated)\n",
        "            # Handle model output\n",
        "            logits = outputs\n",
        "            # Get logits for last token\n",
        "            next_token_logits = logits[0, -1, :]\n",
        "\n",
        "            # Top-k filtering\n",
        "            topk_logits, topk_indices = torch.topk(next_token_logits, top_k)\n",
        "            # Convert to probabilities\n",
        "            probs = torch.softmax(topk_logits, dim=-1)\n",
        "            # Sample from the filtered distribution\n",
        "            next_token = topk_indices[torch.multinomial(probs, 1)]\n",
        "\n",
        "            # Append sampled token to sequence\n",
        "            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "    # Decode and return\n",
        "    print(generated)\n",
        "    return tokenizer.decode(generated[0].tolist())\n"
      ],
      "metadata": {
        "id": "I_yAL7WuCeCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(LLM,tokenizer, \"hello how\", 100, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "lfz9Z6o4ESOj",
        "outputId": "372d3428-8b69-495d-d215-fa91496d9f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[40, 37, 44, 44, 47,  1, 40, 47, 55, 19,  4, 26,  4, 34, 59,  4, 34, 13,\n",
            "         20, 13, 20, 28, 15, 20, 13, 55, 59,  4, 26, 56, 19,  4, 26,  6, 14, 26,\n",
            "         56, 63,  0, 26,  7, 20, 28,  4, 26, 14, 22, 20, 28, 14, 37, 20, 49, 55,\n",
            "         59, 39, 53, 36, 53, 56, 20, 56, 25, 56, 25,  0, 37, 26,  7, 20, 41, 32,\n",
            "         63, 36,  7, 26,  7, 37, 26, 56, 20, 20, 20, 41, 20, 41, 62, 15, 53, 36,\n",
            "         53, 11, 59, 39, 27, 32, 14, 31, 12, 27,  4,  2,  4, 26, 56, 26, 62, 32,\n",
            "         45]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello howJ.R.b—.bDKDKTFKDw—.RxJ.R7ERx…\\nR:KT.REMKTEeKqw—guduxKxPxP\\neR:KiY…d:R:eRxKKKiKi”FuduB—gSYEWCS.,.RxR”Ym'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}