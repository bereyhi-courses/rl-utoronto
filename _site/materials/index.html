<!DOCTYPE html>
<html>

  <head>
  




  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title> Materials - Deep Generative Models / Summer 2025 </title>
  <meta name="description" content="Materials - Deep Generative Models / Summer 2025">
  
  <link rel="stylesheet" href="/genai-utoronto/_css/main.css">
  <link rel="canonical" href="http://localhost:4000/genai-utoronto/materials/">
  <link rel="alternate" type="application/rss+xml" title="Deep Generative Models / Summer 2025 - University of Toronto" href="http://localhost:4000/genai-utoronto/feed.xml" />
<link rel='stylesheet' id='open-sans-css'  href='//fonts.googleapis.com/css?family=Open+Sans%3A300italic%2C400italic%2C600italic%2C300%2C400%2C600&#038;subset=latin%2Clatin-ext&#038;ver=4.2.4' type='text/css' media='all' />
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:600italic,600,400,400italic' rel='stylesheet' type='text/css'>




<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
<link rel="stylesheet" href="/genai-utoronto/assets/css/custom.css">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper" style="z-index: 100;">
      <table><tr>
          <td><img width="75" src="/genai-utoronto/_images/logo.png" valign="middle"></td>
          <td style="padding-left:10px;"><a class="schoolname" style="font-size: 15px; color: black;" class="site-title" href="https://www.ece.utoronto.ca">University of Toronto</a>
          <br/>
          <span class="site-title" style="font-size: 24px;font-weight: bold; color: black; display: block;">Deep Generative Models</span>
          <!-- <span style="color: black; margin-top: -2px;margin-bottom: -5px;" class="site-title" ><a href="/genai-utoronto/" title="Deep Generative Models / Summer 2025 - University of Toronto"><b>Deep Generative Models</a></b></span> -->
          <br/>
          <span class="coursesemeter" style="font-size: 15px;font-weight: bold;margin-top: 10px; color: black; display: block;">Summer 2025</span>
          </td>
        </tr></table>

    <nav class="site-nav">

      <a href="#" class="menu-icon menu.open">
        <svg viewBox="0 0 18 15">
          <path fill="#000000" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#000000" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#000000" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>  

    <div class="trigger"><h1>Main Navigation</h1>

 <ul class="menu">
    
    <li>
        <a class="page-link" href="/genai-utoronto/">
            <i class="fa fa-home fa-lg"></i> Home
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/schedule/">
            <i class="fas fa-calendar-alt"></i> Schedule
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/lectures/">
            <i class="fas fa-video"></i> Lecture Videos
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/materials/">
            <i class="fas fa-book-reader"></i> Materials
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/assignments/">
            <i class="fas fa-book"></i> Assignments
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/project/">
            <i class="fas fa-user-graduate"></i> Project
        </a>
    </li>
    
</ul>

     </div>  
    </nav>

  </div>

  <!-- <div class="header-texture" style="height:100%; z-index: 0; position: absolute; top:0; right: 0; left: 0; 
  background-image: url('/genai-utoronto/_images/pattern.png');" /> -->

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Materials</h1>
  </header>

  <article class="post-content">
    <h2 id="lecture-notes">Lecture Notes</h2>
<p>The lecture notes are uploaded through the semester. For each chapter, the notes are provided section by section.</p>
<h3 id="chapter-0-course-overview-and-logistics">Chapter 0: Course Overview and Logistics</h3>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH0/CH0.pdf">Handouts</a>: All Sections included in a single file</li>
</ul>

<h3 id="chapter-1-text-generation-via-language-models">Chapter 1: Text Generation via Language Models</h3>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec1.pdf">Section 1</a>: Fundamentals of Language Modeling - Primary LMs</li>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec2.pdf">Section 2</a>: Transformer-based LMs</li>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec3.pdf">Section 3</a>: Large Language Models</li>
</ul>

<h3 id="chapter-2-data-generation-problem">Chapter 2: Data Generation Problem</h3>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH2/CH2_Sec1.pdf">Section 1</a>: Basic Definitions</li>
  <li><a href="/genai-utoronto/assets/Notes/CH2/CH2_Sec2.pdf">Section 2</a>: Generative and Discriminative Learning</li>
  <li><a href="/genai-utoronto/assets/Notes/CH2/CH2_Sec3.pdf">Section 3</a>: Generative Modeling</li>
</ul>

<h3 id="chapter-3-data-generation-by-explicit-distribution-learning">Chapter 3: Data Generation by Explicit Distribution Learning</h3>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH3/CH3_Sec1.pdf">Section 1</a>: Distribution Learning</li>
  <li><a href="/genai-utoronto/assets/Notes/CH3/CH3_Sec2.pdf">Section 2</a>: Autoregressive Modeling</li>
  <li><a href="/genai-utoronto/assets/Notes/CH3/CH3_Sec3.pdf">Section 3</a>: Computational Autoregressive Models</li>
  <li><a href="/genai-utoronto/assets/Notes/CH3/CH3_Sec4.pdf">Section 4</a>: Energy-based Models</li>
</ul>

<h2 id="tutorial-notebooks">Tutorial Notebooks</h2>
<p>The tutorial notebooks can be accessed below.</p>
<ul>
  <li><a href="/genai-utoronto/assets/Tutorials/Tutorial_1.ipynb">Tutorial 1</a>: PyTorch Overview, Batch Training, Embedding, and Tokenization, by <strong>Amir Hossein Mobasheri</strong></li>
  <li><a href="/genai-utoronto/assets/Tutorials/Tutorial_2.ipynb">Tutorial 2</a>: Transformers and Large Language Models, by <strong>Amir Hossein Mobasheri</strong></li>
</ul>

<h2 id="book">Book</h2>

<p>There is indeed no single textbook for this course, and we use various resources in the course. Most of resources are research papers, which are included in the reading list below and completed through the semester. The following textbooks have however covered some key notions and related topics.</p>

<ul>
  <li><a href="https://www.bishopbook.com/">[BB] Bishop, Christopher M., and Hugh Bishop. <em>Deep Learning: Foundations and Concepts.</em> Springer Nature, 2023.</a></li>
  <li><a href="https://probml.github.io/pml-book/book2.html">[M] Murphy, Kevin P. <em>Probabilistic Machine Learning: Advanced Topics.</em> MIT Press, 2023.</a></li>
  <li><a href="https://www.deeplearningbook.org/">[GYC] Goodfellow, Ian, et al. <em>Deep Learning.</em> MIT Press, 2016.</a></li>
</ul>

<p>With respect to the first part of the course, the following book provides some good read:</p>

<ul>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]Jurafsky, Dan, and James H. Martin. <em>Speech and Language Processing.</em> 3rd Edition, 2024.</a></li>
</ul>

<p>The following recent textbooks are also good resources for <strong>practicing hands-on skills.</strong> Note that we are <strong>not</strong> simply learning to implement only! We study the fundamentals that led to development of this framework, nowadays known as <strong>generative AI.</strong> Of course, we try to get our hands dirty as well and learn how to do implementation.</p>

<ul>
  <li><a href="https://www.oreilly.com/library/view/hands-on-generative-ai/9781098149239/">Sanseviero, Omar, et al. <em>Hands-On Generative AI with Transformers and Diffusion Models.</em> O’Reilly Media, Inc., 2024.</a></li>
  <li><a href="https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/">Alammar, Jay, and Maarten Grootendorst. <em>Hands-on large language models: language understanding and generation.</em> O’Reilly Media, Inc., 2024.</a></li>
</ul>

<h2 id="reading-list">Reading List</h2>

<p>This section will be completed gradually through the semseter. I will try to break down the essence of each item, so that you could go over them easily.</p>

<h3 id="review">Review</h3>
<p>You may review the idea of Seq2Seq learning in the following references:</p>
<ul>
  <li><a href="https://pdfs.semanticscholar.org/bba8/a2c9b9121e7c78e91ea2a68630e77c0ad20f.pdf">SimpleLM</a>: Initial ideas on making a language model</li>
  <li><a href="https://arxiv.org/abs/1308.0850">SeqGen</a>: Sequence generation via RNNs –<em>Old idea, but yet worth thinking about it!</em></li>
  <li><a href="https://arxiv.org/abs/1409.3215v3">Seq2Seq</a>: How we can do sequence to sequence learning via NNs</li>
</ul>

<p>You may review the idea of transformers in the following resources:</p>
<ul>
  <li><a href="https://arxiv.org/abs/1706.03762">Transformer Paper</a>: Paper <strong>Attention Is All You Need!</strong> published in 2017 that made a great turn in sequence processing</li>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Transformers</a>: Chapter 9 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li><a href="https://www.bishopbook.com/">Transformers</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.1</strong></li>
</ul>

<h3 id="chapter-1-text-generation-and-language-models">Chapter 1: Text Generation and Language Models</h3>
<h4 id="tokenization-and-embedding">Tokenization and Embedding</h4>
<ul>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/2.pdf">Tokenization</a>: Chapter 2 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li>
    <p><a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">Embedding</a>: Chapter 6 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></p>
  </li>
  <li><a href="http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM">Original BPE Algorithm</a>: Original BPE Algorithm proposed by Philip Gage in 1994</li>
  <li><a href="https://arxiv.org/abs/1508.07909">BPE for Tokenization</a>: Paper <em>Neural machine translation of rare words with subword units</em> by <em>Rico Sennrich, Barry Haddow, and Alexandra Birch</em> presented in ACL 2016 that adapted BPE for NLP</li>
</ul>

<h4 id="other-embedding-approaches">Other Embedding Approaches</h4>
<ul>
  <li><a href="https://arxiv.org/abs/1301.3781">Word2Vec</a> Paper <em>Efficient Estimation of Word Representations in Vector Space</em> by <em>Mikolov et al.</em> published in 2013 introducing Word2Vec</li>
  <li><a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe</a> Paper <em>GloVe: Global Vectors for Word Representation</em> by <em>Pennington</em> et al._ published in 2014 introducing GloVe</li>
  <li><a href="https://arxiv.org/abs/1609.08144">WordPiece</a>: Paper <em>Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</em> by <em>Yonghui Wu et al.</em> published in 2016 introducing WordPiece (used in BERT)</li>
  <li><a href="https://arxiv.org/abs/1808.06226">SentencePiece</a>: Paper <em>SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</em> by <em>Taku Kudo and John Richardson</em> presented in EMNLP 2018 that introduces a language-independent tokenizer</li>
  <li><a href="https://arxiv.org/abs/1802.05365">ELMo</a> Paper <em>Deep contextualized word representations</em> by <em>Peters et al.</em> introducing ELMo <strong>a context-sensitive embedding</strong></li>
  <li><a href="https://arxiv.org/abs/2105.13626">ByT5</a>: Paper <em>ByT5: Towards a token-free future with pre-trained byte-to-byte models</em> by <em>Xue et al.</em> presented in ACL 2022 proposing ByT5</li>
</ul>

<h4 id="language-modelling">Language Modelling</h4>
<ul>
  <li><a href="https://www.bishopbook.com/">LMs</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.2</strong></li>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-Gram LMs</a>: Chapter 3 of <em>Speech and Language Processing;</em> <strong>Section 3.1</strong> on N-gram LM</li>
  <li><a href="https://www.bishopbook.com/">Maximum Likelihood</a>: Chapter 2 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Sections 12.1 – 12.3</strong></li>
</ul>

<h4 id="recurrent-lms">Recurrent LMs</h4>
<ul>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/8.pdf">Recurrent LMs</a>: Chapter 8 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li><a href="https://arxiv.org/abs/1708.02182">LSTM LMs</a>: Paper <em>Regularizing and Optimizing LSTM Language Models</em> by <em>Stephen Merity, Nitish Shirish Keskar, and Richard Socher</em> published in ICLR 2018 enabling LSTMs to perform strongly on word-level language modeling</li>
  <li><a href="https://arxiv.org/abs/1711.03953">High-Rank Recurrent LMs</a>: Paper <em>Breaking the Softmax Bottleneck: A High-Rank RNN Language Model</em> by <em>Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen</em> presented at ICLR 2018 proposing Mixture of Softmaxes (MoS) and achieving state-of-the-art results at the time</li>
</ul>

<h4 id="transformer-based-lms-and-llms">Transformer-based LMs and LLMs</h4>
<ul>
  <li><a href="https://www.bishopbook.com/">Transformer LMs</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.3</strong></li>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf">LLMs via Transformers</a>: Chapter 10 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
</ul>

<h4 id="gpts">GPTs</h4>
<ul>
  <li><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT-1</a>: Paper <em>Improving Language Understanding by Generative Pre-Training</em> by <em>Alec Radford et al.</em> (OpenAI, 2018) that introduced GPT-1 and revived the idea of pretraining transformers as LMs followed by supervised fine-tuning</li>
  <li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>: Paper <em>Language Models are Unsupervised Multitask Learners</em> by <em>Alec Radford et al.</em> (OpenAI, 2019) that introduces GPT-2 with 1.5B parameter trained on web text</li>
  <li><a href="https://arxiv.org/abs/2005.14165">GPT-3</a>: Paper <em>Language Models are Few-Shot Learners</em> by <em>Tom B. Brown et al.</em> (OpenAI, 2020) that introduces GPT-3, a 175B-parameter transformer LM</li>
  <li><a href="https://arxiv.org/abs/2303.08774">GPT-4</a>: <em>GPT-4 Technical Report</em> by <em>OpenAI</em> (2023) that provides an overview of GPT-4’s capabilities</li>
</ul>

<h4 id="other-llms">Other LLMs</h4>
<ul>
  <li><a href="https://arxiv.org/abs/1810.04805">BERT</a>: Paper <em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em> by <em>Jacob Devlin et al.</em> presented at NAACL 2019 that introduced BERT
<!-- * [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237): Paper _XLNet: Generalized Autoregressive Pretraining for Language Understanding_ by _Zhilin Yang et al._ presented at NeurIPS 2019 that introduces XLNet --></li>
  <li><a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>: Paper <em>RoBERTa: A Robustly Optimized BERT Pretraining Approach</em> by <em>Yinhan Liu, et al.</em> (Facebook AI, 2019) that shows BERT’s performance can be significantly improved by more data, longer training, and removing next sentence prediction</li>
  <li><a href="https://arxiv.org/abs/1910.10683">T5</a>: Paper <em>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</em> by <em>Colin Raffel et al.</em> (JMLR 2020) that reformulates all NLP tasks as text-to-text problems introducing the T5 model</li>
</ul>

<h4 id="data-for-llms">Data for LLMs</h4>
<ul>
  <li><a href="https://arxiv.org/abs/2101.00027">The Pile</a>: Paper <em>The Pile: An 800GB Dataset of Diverse Text for Language Modeling</em> by <em>Leo Gao et al.</em> presented in 2020 introductin dataset <strong>The Pile</strong></li>
  <li><a href="https://arxiv.org/abs/1704.04683">RACE</a>: Paper <em>RACE: Large-scale Reading Comprehension Dataset from Examinations</em> by <em>Guokun Lai et al.</em> presented at EMNLP in 2017 introducing a large-scale dataset of English reading comprehension questions from real-world exams</li>
  <li><a href="https://arxiv.org/abs/1511.06398">BookCorpus</a>: Paper <em>Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books</em> by <em>Yukun Zhu et al.</em> presented at ICCV in 2015 introducing the dataset BookCorpus. It was used to pre-train GPT-1 and BERT; nevertheless, it turned out that the dataset was collected without authors consent; see <a href="https://en.wikipedia.org/wiki/BookCorpus">the Wikipedia article</a>. It was hence replaced later with BookCorpusOpen</li>
  <li><a href="https://arxiv.org/abs/2105.05241">Documentation Debt</a>: Paper <em>Addressing “Documentation Debt” in Machine Learning Research: A Retrospective Datasheet for BookCorpus</em> by <em>Jack Bandy and Nicholas Vincent</em> published in 2021 discussing the efficiency and legality of data collection by looking into <a href="https://arxiv.org/abs/1511.06398">BookCorpus</a></li>
</ul>

<h4 id="earlier-work-on-pretraining">Earlier Work on Pretraining</h4>
<ul>
  <li><a href="https://arxiv.org/abs/1511.01432">SSL</a>: Paper <em>Semi-supervised Sequence Learning</em> by <em>Andrew M. Dai et al.</em> published in 2015 that explores using unsupervised pretraining followed by supervised fine-tuning; this was an early solid work advocating <strong>pre-training</strong> idea for LMs</li>
  <li><a href="https://arxiv.org/abs/1801.06146">ULMFiT</a>: Paper <em>Universal Language Model Fine-tuning for Text Classification</em> by <em>Jeremy Howard et al.</em> presented at ACL in 2018 introducing ULMFiT that uses pre-trained LMs with task-specific fine-tuning</li>
</ul>

<h4 id="fine-tuning">Fine-tuning</h4>
<ul>
  <li><a href="https://www.bishopbook.com/">LMs</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.3.5</strong></li>
  <li><a href="https://arxiv.org/abs/2106.09685">LoRA</a>: Paper <em>LoRA: Low-Rank Adaptation of Large Language Models</em> by <em>Edward J. Hu et al.</em> presented at ICLR in 2022 introducing LoRA</li>
  <li><a href="https://arxiv.org/abs/2404.03592">ReFT</a>: Paper <em>ReFT: Representation Finetuning for Language Models</em> by <em>Z. Wu et al.</em> presented at NeurIPS in 2024 proposing an alternative fine-tuning algorithm</li>
</ul>

<h4 id="prompt-design">Prompt Design</h4>
<ul>
  <li><a href="https://arxiv.org/abs/1707.00600">Zero-Shot</a>: Paper <em>Zero-shot Learning — A Comprehensive Evaluation of the Good, the Bad and the Ugly</em> by <em>Yongqin Xian et al.</em> at IEEE Tran. PAMI in 2018 presenting an overview on zero-shot learning</li>
  <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought</a>: Paper <em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em> by <em>Jason Wei et al.</em> presented at NeurIPS in 2022 introducing <strong>chain-of-thought</strong> prompting</li>
  <li><a href="https://arxiv.org/abs/2101.00190">Prefix-Tuning</a>: Paper <em>Prefix-Tuning: Optimizing Continuous Prompts for Generation</em> by <em>Xiang Lisa Li et al.</em> presented at ACL in 2021 proposing prefix-tuning approach for prompting</li>
  <li><a href="https://arxiv.org/abs/2104.08691">Prompt-Tuning</a>: Paper <em>The Power of Scale for Parameter-Efficient Prompt Tuning</em> by <em>B. Lester et al.</em> presented at EMNLP in 2021 proposing the prompt tuning idea, i.e., learning to prompt</li>
  <li><a href="https://arxiv.org/abs/2205.11916">Zero-Shot LLMs</a>: Paper <em>Large Language Models are Zero-Shot Reasoners</em> by <em>T. Kojima et al.</em> presented at NeurIPS in 2022 studying zero-shot learning with LLMs</li>
  <li><a href="https://spectrum.ieee.org/prompt-engineering-is-dead">Prompt Engineering is Dead</a>: Article <em>AI Prompt Engineering Is Dead: Long Live AI Prompt Engineering</em> by <em>Dina Genkina</em> published in <em>IEEE Spectrum</em> in 2024</li>
</ul>

<h4 id="foundation-models">Foundation Models</h4>
<ul>
  <li><a href="https://crfm.stanford.edu/">CRFM</a> Center for Research on Foundation Models who coined the term <em>Foundation Model</em></li>
</ul>

<h3 id="chapter-2-data-generation-problem-1">Chapter 2: Data Generation Problem</h3>
<h4 id="basic-definitions">Basic Definitions</h4>
<ul>
  <li><a href="https://www.bishopbook.com/">Probabilistic Model</a>: Chapter 2 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Sections 2.4 to 2.6</strong></li>
  <li><a href="https://probml.github.io/pml-book/book2.html">Statistics</a>: Chapter 3 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a> <strong>Sections 3.1 to 3.3</strong></li>
  <li><a href="https://www.deeplearningbook.org/">Bayesian Statistics</a>: Chapter 5 of <a href="https://www.deeplearningbook.org/">[GYC]</a> <strong>Section 5.6</strong></li>
</ul>

<h4 id="generative-and-discriminative-learning">Generative and Discriminative Learning</h4>
<ul>
  <li><a href="https://www.bishopbook.com/">Discriminative and Generative Models</a>: Chapter 5 of <a href="https://www.bishopbook.com/">[BB]</a></li>
</ul>

<h4 id="generative-models">Generative Models</h4>
<ul>
  <li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2001.tb00465.x?casa_token=DH9SI9elEXQAAAAA%3AVgLUtFs8TJVMldMvLbOhXTuvkyubn3CDcSaE7xD9fe02YwcTwBik5fEpAY1SpcMvl0kJZuwHqrKbIA">Naive Bayes</a>: Paper <em>Idiot’s Bayes—Not So Stupid After All?</em> by <em>D. Hand and K. Yu</em> published at <em>Statistical Review</em> in 2001 discussing the efficiency of Naive Bayes for classification</li>
  <li><a href="https://proceedings.neurips.cc/paper/2001/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html">Naive Bayes vs Linear Regression</a>: Paper <em>On Discriminative vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes</em> by <em>A. Ng and M. Jordan</em> presented at <em>NeurIPS</em> in 2001 elaborating the data-efficiency efficiency of Naive Bayes and asymptotic superiority of Logistic Regression</li>
  <li><a href="https://probml.github.io/pml-book/book2.html">Generative Models – Overview</a>: Chapter 20 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a> <strong>Sections 20.1 to 20.3</strong></li>
</ul>

<h3 id="chapter-3-explicit-distribution-learning">Chapter 3: Explicit Distribution Learning</h3>

<h4 id="sampling">Sampling</h4>
<ul>
  <li><a href="https://www.bishopbook.com/">Sampling Overview</a>: Chapter 14 of <a href="https://www.bishopbook.com/">[BB]</a></li>
  <li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Sampling</a> The book <em>Pattern Recognition and Machine Learning</em> by Christopher Bishop. Read <strong>Chapter 11</strong> to know about how challenging <em>sampling from a distribution</em> is</li>
  <li><a href="https://www.deeplearningbook.org/">Sampling Methods</a>: Chapter 17 of <a href="https://www.deeplearningbook.org/">[GYC]</a> <strong>Sections 17.1 and 17.2</strong></li>
</ul>

<h4 id="distribution-learning-via-mle">Distribution Learning via MLE</h4>
<ul>
  <li><a href="https://probml.github.io/pml-book/book2.html">KL Divergence and MLE</a>: Chapter 5 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a> <strong>Sections 5.1 to 5.2</strong></li>
  <li><a href="https://www.deeplearningbook.org/">MLE</a>: Chapter 5 of <a href="https://www.deeplearningbook.org/">[GYC]</a> <strong>Section 5.5</strong></li>
  <li><a href="http://www.inference.org.uk/itprnn/book.pdf">Maximum Likelihood Learning</a> The book <em>Information Theory, Inference, and Learning Algorithms</em> by David MacKay which discusses MLE for clustering in <strong>Chapter 22</strong></li>
  <li><a href="https://probml.github.io/pml-book/book2.html">Evaluating Distribution Learning</a>: Chapter 20 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a> <strong>Sections 20.4</strong></li>
</ul>

<h4 id="autoregressive-models">Autoregressive Models</h4>
<ul>
  <li><a href="https://probml.github.io/pml-book/book2.html">Autoregressive Models</a>: Chapter 22 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a></li>
  <li><a href="https://arxiv.org/abs/1601.06759">PixelRNN and PixelCNN</a>: Paper <em>Pixel Recurrent Neural Networks</em> by <em>A. Oord et al.</em> presented at <em>ICMLR</em> in 2016 proposing PixelRNN and PixelCNN</li>
  <li><a href="https://proceedings.mlr.press/v119/chen20s/chen20s.pdf">ImageGPT</a>: Paper <em>Generative Pretraining from Pixels</em> by <em>M. Chen et al.</em> presented at <em>ICML</em> in 2020 proposing ImageGPT</li>
</ul>

<h4 id="energy-based-models">Energy-based Models</h4>
<ul>
  <li><a href="https://www.bishopbook.com/">EBMs</a>: Chapter 24 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a></li>
  <li><a href="https://www.deeplearningbook.org/">Partition Function and Normalizing</a>: Chapter 16 of <a href="https://www.deeplearningbook.org/">[GYC]</a> <strong>Section 16.2</strong></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/6796877">Universality of EBMs</a> Paper <em>Representational power of restricted Boltzmann machines and deep belief networks,</em> by <em>N. Le Roux and Y. Bengio</em> published at <em>Neural Computation</em> in 2008 elaborating the representational power of EBMs
*<a href="https://www.researchgate.net/profile/Raia-Hadsell/publication/200744586_A_tutorial_on_energy-based_learning/links/5694442c08aeab58a9a2e650/A-tutorial-on-energy-based-learning.pdf">Tutorial on EBMs</a> Survey <em>A Tutorial on Energy-Based Learning,</em> by <em>Y. LeCun et al.</em> published in 2006</li>
</ul>

<h4 id="mcmc-sampling">MCMC Sampling</h4>
<ul>
  <li><a href="https://www.bishopbook.com/">MCMC Algorithms</a>: Chapter 12 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a> <strong>Sections 12.3, 12.6 and 12.7</strong></li>
  <li><a href="https://www.bishopbook.com/">Gibbs Sampling and Langevin</a>: Chapter 14 of <a href="https://www.bishopbook.com/">[BB]</a></li>
</ul>

<h4 id="training-ebms-by-mcmc-sampling">Training EBMs by MCMC Sampling</h4>
<ul>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/6796877">Conservative Divergence</a> Paper <em>Training Products of Experts by Minimizing Contrastive Divergence,</em> by <em>G. Hinton</em> published at <em>Neural Computation</em> in 2002 proposing the idea of <em>Conservative Divergence</em></li>
  <li><a href="https://arxiv.org/abs/1903.08689">Training by MCMC</a> Paper <em>Implicit Generation and Generalization in Energy-Based Models</em> published by <em>Y. Du and I. Mordatch</em> in <em>NeurIPS</em> 2019 discussing efficiency of MCMC algorithms for EBM training</li>
  <li><a href="https://arxiv.org/abs/1903.08689">Improved CD</a> Paper <em>Improved Contrastive Divergence Training of Energy-Based Models</em> published by <em>Y. Du et al.</em> in <em>ICML</em> 2021 proposing an efficient training based on Hinton’s CD ideal</li>
  <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/5973">Anatomy of MCMC</a> Paper <em>On the Anatomy of MCMC-Based Maximum Likelihood Learning of Energy-Based Models</em> published by <em>E. Nijkamp et al.</em> in <em>AAAI</em> 2020 looking on the stability of training by MCMC algorithms</li>
</ul>


  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

<!--     <h2 class="footer-heading">University of Toronto</h2> -->
         <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
 

         <p class="text">
The Edward S. Rogers Sr. Department of Electrical and Computer Engineering<br />
University of Toronto<br />

      </div>

      <div class="footer-col  footer-col-2">
       <ul class="social-media-list">
     

          

          
  <li>
    <a href="https://www.bereyhi.com">
      <i class="fas fa-globe" style="color:gray"></i> bereyhi.com
    </a>
  </li>


          

          

          
  <li>
    <a href="https://www.ece.utoronto.ca">
      <i class="fas fa-globe" style="color:gray"></i> ece.utoronto.ca
    </a>
  </li>




       
        </ul>
      </div>
    </div>

  </div>

</footer>

  </body>

</html>
<!-- d.s.m.s.050600.062508.030515.080516.030818 | "Baby, I'm Yours" -->