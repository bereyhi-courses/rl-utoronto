<!DOCTYPE html>
<html>

  <head>
  




  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title> Schedule - Deep Generative Models / Summer 2025 </title>
  <meta name="description" content="Schedule - Deep Generative Models / Summer 2025">
  
  <link rel="stylesheet" href="/genai-utoronto/_css/main.css">
  <link rel="canonical" href="http://localhost:4000/genai-utoronto/schedule/">
  <link rel="alternate" type="application/rss+xml" title="Deep Generative Models / Summer 2025 - University of Toronto" href="http://localhost:4000/genai-utoronto/feed.xml" />
<link rel='stylesheet' id='open-sans-css'  href='//fonts.googleapis.com/css?family=Open+Sans%3A300italic%2C400italic%2C600italic%2C300%2C400%2C600&#038;subset=latin%2Clatin-ext&#038;ver=4.2.4' type='text/css' media='all' />
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:600italic,600,400,400italic' rel='stylesheet' type='text/css'>




<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
<link rel="stylesheet" href="/genai-utoronto/assets/css/custom.css">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper" style="z-index: 100;">
      <table><tr>
          <td><img width="75" src="/genai-utoronto/_images/logo.png" valign="middle"></td>
          <td style="padding-left:10px;"><a class="schoolname" style="font-size: 15px; color: black;" class="site-title" href="https://www.ece.utoronto.ca">University of Toronto</a>
          <br/>
          <span class="site-title" style="font-size: 24px;font-weight: bold; color: black; display: block;">Deep Generative Models</span>
          <!-- <span style="color: black; margin-top: -2px;margin-bottom: -5px;" class="site-title" ><a href="/genai-utoronto/" title="Deep Generative Models / Summer 2025 - University of Toronto"><b>Deep Generative Models</a></b></span> -->
          <br/>
          <span class="coursesemeter" style="font-size: 15px;font-weight: bold;margin-top: 10px; color: black; display: block;">Summer 2025</span>
          </td>
        </tr></table>

    <nav class="site-nav">

      <a href="#" class="menu-icon menu.open">
        <svg viewBox="0 0 18 15">
          <path fill="#000000" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#000000" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#000000" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>  

    <div class="trigger"><h1>Main Navigation</h1>

 <ul class="menu">
    
    <li>
        <a class="page-link" href="/genai-utoronto/">
            <i class="fa fa-home fa-lg"></i> Home
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/schedule/">
            <i class="fas fa-calendar-alt"></i> Schedule
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/lectures/">
            <i class="fas fa-video"></i> Lecture Videos
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/materials/">
            <i class="fas fa-book-reader"></i> Materials
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/assignments/">
            <i class="fas fa-book"></i> Assignments
        </a>
    </li>
    
    <li>
        <a class="page-link" href="/genai-utoronto/project/">
            <i class="fas fa-user-graduate"></i> Project
        </a>
    </li>
    
</ul>

     </div>  
    </nav>

  </div>

  <!-- <div class="header-texture" style="height:100%; z-index: 0; position: absolute; top:0; right: 0; left: 0; 
  background-image: url('/genai-utoronto/_images/pattern.png');" /> -->

</header>


    <div class="page-content">
      <div class="wrapper">
        <header class="post-header">
    <h1 class="post-title">Schedule</h1>
</header>



<div class="home" style="font-size: 0.8em;">
    <ul class="responsive-table" style="margin-left: 0;">
        <li class="table-header">
            <div class="col col-1">Event</div>
            <div class="col col-1-2">Date</div>
            <div class="col col-2">Description</div>
            <div class="col col-4">Description</div>
        </li>
        
        

        
        
        
        

        
        
        
        
        
        
        
        
        
        
        
        
        
        
        <li class="table-row table-row-raw_event">
            <div class="col col-1">Session</div>
<div class="col col-1-2" data-label="Date">
    05/06/2025
    
    18:00
    
    <br class="date-spliter" />Tuesday
</div>
<div class="col col-2" data-label="Description">First Lecture</div>
<div class="col col-4 markdown-content" data-label="Course Material">
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/06/2025
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" data-label="Description">
    Lecture 0: Course Overview and Logistics
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH0/CH0.pdf">Chapter 0</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/06/2025
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" data-label="Description">
    Lecture 1: Tokenization and Embedding
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec1.pdf">Chapter 1 - Section 1</a> Pgs 1:18
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/2.pdf">Tokenization</a>: Chapter 2 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">Embedding</a>: Chapter 6 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li><a href="http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM">Original BPE Algorithm</a>: Original BPE Algorithm proposed by Philip Gage in 1994</li>
  <li><a href="https://arxiv.org/abs/1508.07909">BPE for Tokenization</a>: Paper <em>Neural machine translation of rare words with subword units</em> by <em>Rico Sennrich, Barry Haddow, and Alexandra Birch</em> presented in ACL 2016 that adapted BPE for NLP</li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/08/2025
    <br class="date-spliter"/>Thursday
</div>
<div class="col col-2" data-label="Description">
    Lecture 2: Language Distribution and Bi-Gram Model
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec1.pdf">Chapter 1 - Section 1</a> Pgs 18:32
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://www.bishopbook.com/">LMs</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.2</strong></li>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">N-Gram LMs</a>: Chapter 3 of <em>Speech and Language Processing;</em> <strong>Section 3.1</strong> on N-gram LM</li>
  <li><a href="https://www.bishopbook.com/">Maximum Likelihood</a>: Chapter 2 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Sections 12.1 – 12.3</strong></li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/08/2025
    <br class="date-spliter"/>Thursday
</div>
<div class="col col-2" data-label="Description">
    Lecture 3: Recurrent LMs
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec1.pdf">Chapter 1 - Section 1</a> Pgs 32:42
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/8.pdf">Recurrent LMs</a>: Chapter 8 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li><a href="https://arxiv.org/abs/1708.02182">LSTM LMs</a>: Paper <em>Regularizing and Optimizing LSTM Language Models</em> by <em>Stephen Merity, Nitish Shirish Keskar, and Richard Socher</em> published in ICLR 2018 enabling LSTMs to perform strongly on word-level language modeling</li>
  <li><a href="https://arxiv.org/abs/1711.03953">High-Rank Recurrent LMs</a>: Paper <em>Breaking the Softmax Bottleneck: A High-Rank RNN Language Model</em> by <em>Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen</em> presented at ICLR 2018 proposing Mixture of Softmaxes (MoS) and achieving state-of-the-art results at the time</li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/13/2025
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" data-label="Description">
    Lecture 4: Context Extraction via Self-Attention
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec2.pdf">Chapter 1 - Section 2</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://arxiv.org/abs/1706.03762">Transformer Paper</a>: Paper <strong>Attention Is All You Need!</strong> published in 2017 that made a great turn in sequence processing</li>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Transformers</a>: Chapter 9 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li><a href="https://www.bishopbook.com/">Transformers</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.1</strong></li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/13/2025
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" data-label="Description">
    Lecture 5: Transformer LM
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec2.pdf">Chapter 1 - Section 2</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://www.bishopbook.com/">Transformer LMs</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.3</strong></li>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf">LLMs via Transformers</a>: Chapter 10 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/15/2025
    <br class="date-spliter"/>Thursday
</div>
<div class="col col-2" data-label="Description">
    Lecture 6: LLM Examples
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec3.pdf">Chapter 1 - Section 3</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT-1</a>: Paper <em>Improving Language Understanding by Generative Pre-Training</em> by <em>Alec Radford et al.</em> (OpenAI, 2018) that introduced GPT-1 and revived the idea of pretraining transformers as LMs followed by supervised fine-tuning</li>
  <li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>: Paper <em>Language Models are Unsupervised Multitask Learners</em> by <em>Alec Radford et al.</em> (OpenAI, 2019) that introduces GPT-2 with 1.5B parameter trained on web text</li>
  <li><a href="https://arxiv.org/abs/2005.14165">GPT-3</a>: Paper <em>Language Models are Few-Shot Learners</em> by <em>Tom B. Brown et al.</em> (OpenAI, 2020) that introduces GPT-3, a 175B-parameter transformer LM</li>
  <li>
    <p><a href="https://arxiv.org/abs/2303.08774">GPT-4</a>: <em>GPT-4 Technical Report</em> by <em>OpenAI</em> (2023) that provides an overview of GPT-4’s capabilities</p>
  </li>
  <li><a href="https://arxiv.org/abs/2101.00027">The Pile</a>: Paper <em>The Pile: An 800GB Dataset of Diverse Text for Language Modeling</em> by <em>Leo Gao et al.</em> presented in 2020 introductin dataset <strong>The Pile</strong></li>
  <li><a href="https://arxiv.org/abs/2105.05241">Documentation Debt</a>: Paper <em>Addressing “Documentation Debt” in Machine Learning Research: A Retrospective Datasheet for BookCorpus</em> by <em>Jack Bandy and Nicholas Vincent</em> published in 2021 discussing the efficiency and legality of data collection by looking into <a href="https://arxiv.org/abs/1511.06398">BookCorpus</a></li>
</ul>


    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/15/2025
    <br class="date-spliter"/>Thursday
</div>
<div class="col col-2" data-label="Description">
    Lecture 7: Pre-training vs Fine-tuning
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec3.pdf">Chapter 1 - Section 3</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://arxiv.org/abs/1511.01432">SSL</a>: Paper <em>Semi-supervised Sequence Learning</em> by <em>Andrew M. Dai et al.</em> published in 2015 that explores using unsupervised pretraining followed by supervised fine-tuning; this was an early solid work advocating <strong>pre-training</strong> idea for LMs</li>
  <li><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT-1</a>: Paper <em>Improving Language Understanding by Generative Pre-Training</em> by <em>Alec Radford et al.</em> (OpenAI, 2018) that introduced GPT-1 and revived the idea of pretraining transformers as LMs followed by supervised fine-tuning</li>
</ul>


    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/15/2025
    <br class="date-spliter"/>Thursday
</div>
<div class="col col-2" data-label="Description">
    Lecture 8: Statistical View and LoRA
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec3.pdf">Chapter 1 - Section 3</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://www.bishopbook.com/">LMs</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.3.5</strong></li>
  <li><a href="https://arxiv.org/abs/2106.09685">LoRA</a>: Paper <em>LoRA: Low-Rank Adaptation of Large Language Models</em> by <em>Edward J. Hu et al.</em> presented at ICLR in 2022 introducing LoRA</li>
</ul>


    
</div>
        </li>
        
        
        
        <li class="table-row table-row-assignment">
            <div class="col col-1" style="color: #31708f ;font-weight: bold;" data-label="Event">Assignment</div>
<div class="col col-1-2" data-label="Date">
    05/20/2025
    <br class="date-spliter" />Tuesday
</div>
<div class="col col-2" style="font-weight: bold" data-label="Description">
    Assignment #1 - Language Modeling released!
</div>
<div class="col col-4" data-label="Course Material">
    [<a href="/genai-utoronto/assignments/01_assignment1">Assignment #1 - Language Modeling</a>]
    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/20/2025
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" data-label="Description">
    Lecture 9: Prompt Design
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec3.pdf">Chapter 1 - Section 3</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought</a>: Paper <em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em> by <em>Jason Wei et al.</em> presented at NeurIPS in 2022 introducing <strong>chain-of-thought</strong> prompting</li>
  <li><a href="https://arxiv.org/abs/2101.00190">Prefix-Tuning</a>: Paper <em>Prefix-Tuning: Optimizing Continuous Prompts for Generation</em> by <em>Xiang Lisa Li et al.</em> presented at ACL in 2021 proposing prefix-tuning approach for prompting</li>
  <li><a href="https://arxiv.org/abs/2104.08691">Prompt-Tuning</a>: Paper <em>The Power of Scale for Parameter-Efficient Prompt Tuning</em> by <em>B. Lester et al.</em> presented at EMNLP in 2021 proposing the prompt tuning idea, i.e., learning to prompt</li>
  <li><a href="https://arxiv.org/abs/2205.11916">Zero-Shot LLMs</a>: Paper <em>Large Language Models are Zero-Shot Reasoners</em> by <em>T. Kojima et al.</em> presented at NeurIPS in 2022 studying zero-shot learning with LLMs</li>
</ul>


    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/20/2025
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" data-label="Description">
    Lecture 10: Data Generation Problem - Basic Definitions
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH2/CH2_Sec1.pdf">Chapter 2 - Section 1</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://www.bishopbook.com/">Probabilistic Model</a>: Chapter 2 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Sections 2.4 to 2.6</strong></li>
  <li><a href="https://probml.github.io/pml-book/book2.html">Statistics</a>: Chapter 3 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a> <strong>Sections 3.1 to 3.3</strong></li>
</ul>


    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/22/2025
    <br class="date-spliter"/>Thursday
</div>
<div class="col col-2" data-label="Description">
    Lecture 11: Discriminative vs Generative Learning
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH2/CH2_Sec2.pdf">Chapter 2 - Section 2</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://www.bishopbook.com/">Discriminative and Generative Models</a>: Chapter 5 of <a href="https://www.bishopbook.com/">[BB]</a></li>
</ul>


    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/22/2025
    <br class="date-spliter"/>Thursday
</div>
<div class="col col-2" data-label="Description">
    Lecture 12: Naive Bayes - Most Basic Generative Model
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH2/CH2_Sec3.pdf">Chapter 2 - Section 3</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2001.tb00465.x?casa_token=DH9SI9elEXQAAAAA%3AVgLUtFs8TJVMldMvLbOhXTuvkyubn3CDcSaE7xD9fe02YwcTwBik5fEpAY1SpcMvl0kJZuwHqrKbIA">Naive Bayes</a>: Paper <em>Idiot’s Bayes—Not So Stupid After All?</em> by <em>D. Hand and K. Yu</em> published at <em>Statistical Review</em> in 2001 discussing the efficiency of Naive Bayes for classification</li>
  <li><a href="https://proceedings.neurips.cc/paper/2001/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html">Naive Bayes vs Linear Regression</a>: Paper <em>On Discriminative vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes</em> by <em>A. Ng and M. Jordan</em> presented at <em>NeurIPS</em> in 2001 elaborating the data-efficiency efficiency of Naive Bayes and asymptotic superiority of Logistic Regression</li>
  <li><a href="https://probml.github.io/pml-book/book2.html">Generative Models – Overview</a>: Chapter 20 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a> <strong>Sections 20.1 to 20.3</strong></li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/27/2025
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" data-label="Description">
    Lecture 13: Explicit Distribution Learning - Sampling
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH3/CH3_Sec1.pdf">Chapter 3 - Section 1</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://www.bishopbook.com/">Sampling Overview</a>: Chapter 14 of <a href="https://www.bishopbook.com/">[BB]</a></li>
  <li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Sampling</a> The book <em>Pattern Recognition and Machine Learning</em> by Christopher Bishop. Read <strong>Chapter 11</strong> to know about how challenging <em>sampling from a distribution</em> is</li>
  <li><a href="https://www.deeplearningbook.org/">Sampling Methods</a>: Chapter 17 of <a href="https://www.deeplearningbook.org/">[GYC]</a> <strong>Sections 17.1 and 17.2</strong></li>
</ul>


    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/27/2025
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" data-label="Description">
    Lecture 14: Maximum Likelihood Learning
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH3/CH3_Sec1.pdf">Chapter 3 - Section 1</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://probml.github.io/pml-book/book2.html">KL Divergence and MLE</a>: Chapter 5 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a> <strong>Sections 5.1 to 5.2</strong></li>
  <li><a href="https://www.deeplearningbook.org/">MLE</a>: Chapter 5 of <a href="https://www.deeplearningbook.org/">[GYC]</a> <strong>Section 5.5</strong></li>
  <li><a href="http://www.inference.org.uk/itprnn/book.pdf">Maximum Likelihood Learning</a> The book <em>Information Theory, Inference, and Learning Algorithms</em> by David MacKay which discusses MLE for clustering in <strong>Chapter 22</strong></li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/27/2025
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" data-label="Description">
    Lecture 15: Autoregressive Modeling
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH3/CH3_Sec2.pdf">Chapter 3 - Section 2</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://probml.github.io/pml-book/book2.html">Autoregressive Models</a>: Chapter 22 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a></li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/29/2025
    <br class="date-spliter"/>Thursday
</div>
<div class="col col-2" data-label="Description">
    Lecture 16: Computational AR Models
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH3/CH3_Sec2.pdf">Chapter 3 - Section 2</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://probml.github.io/pml-book/book2.html">Autoregressive Models</a>: Chapter 22 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a></li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    05/29/2025
    <br class="date-spliter"/>Thursday
</div>
<div class="col col-2" data-label="Description">
    Lecture 17: PixelRNN
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH3/CH3_Sec3.pdf">Chapter 3 - Section 3</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://arxiv.org/abs/1601.06759">PixelRNN and PixelCNN</a>: Paper <em>Pixel Recurrent Neural Networks</em> by <em>A. Oord et al.</em> presented at <em>ICML</em> in 2016 proposing PixelRNN and PixelCNN</li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    06/03/2025
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" data-label="Description">
    Lecture 18: Masked AR Models - PixelCNN and ImageGPT
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH3/CH3_Sec3.pdf">Chapter 3 - Section 3</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://arxiv.org/abs/1601.06759">PixelRNN and PixelCNN</a>: Paper <em>Pixel Recurrent Neural Networks</em> by <em>A. Oord et al.</em> presented at <em>ICML</em> in 2016 proposing PixelRNN and PixelCNN</li>
  <li><a href="https://proceedings.mlr.press/v119/chen20s/chen20s.pdf">ImageGPT</a>: Paper <em>Generative Pretraining from Pixels</em> by <em>M. Chen et al.</em> presented at <em>ICML</em> in 2020 proposing ImageGPT</li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-lecture">
            <div class="col col-1" data-label="Event">Lecture</div>
<div class="col col-1-2" data-label="Date">
    06/03/2025
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" data-label="Description">
    Lecture 19: Energy Based Models - Boltzmann Distribution
    <div class="schedule-lecture-links">
    
    

    </div>
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <!--  -->
    
    <p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH3/CH3_Sec4.pdf">Chapter 3 - Section 4</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://www.bishopbook.com/">EBMs</a>: Chapter 24 of <a href="https://probml.github.io/pml-book/book2.html">[M]</a></li>
  <li><a href="https://www.deeplearningbook.org/">Partition Function and Normalizing</a>: Chapter 16 of <a href="https://www.deeplearningbook.org/">[GYC]</a> <strong>Section 16.2</strong></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/6796877">Universality of EBMs</a> Paper <em>Representational power of restricted Boltzmann machines and deep belief networks,</em> by <em>N. Le Roux and Y. Bengio</em> published at <em>Neural Computation</em> in 2008 elaborating the representational power of EBMs
*<a href="https://www.researchgate.net/profile/Raia-Hadsell/publication/200744586_A_tutorial_on_energy-based_learning/links/5694442c08aeab58a9a2e650/A-tutorial-on-energy-based-learning.pdf">Tutorial on EBMs</a> Survey <em>A Tutorial on Energy-Based Learning,</em> by <em>Y. LeCun et al.</em> published in 2006</li>
</ul>

    
</div>
        </li>
        
        
        
        <li class="table-row table-row-due">
            <div class="col col-1" style="color: #8a6d3b ;font-weight: bold;" data-label="Event">Due</div>
<div class="col col-1-2" style="font-weight: bold" data-label="Date">
    06/05/2025
    23:59
    <br class="date-spliter" />Thursday
</div>
<div class="col col-2" style="font-weight: bold" data-label="Description">
    Assignment #1 due
</div>
<div class="col col-4" data-label="Course Material">
</div>
        </li>
        
        
        
        <li class="table-row table-row-exam">
            <div class="col col-1" style="color: #a94442 ;font-weight: bold;" data-label="Event">Exam</div>
<div class="col col-1-2" style="font-weight: bold" data-label="Date">
    06/24/2025
    18:00
    <br class="date-spliter"/>Tuesday
</div>
<div class="col col-2" style="font-weight: bold" data-label="Description">
    Midterm
</div>
<div class="col col-4 markdown-content" data-label="Course Material">
    <p><strong>Topics:</strong></p>
<ul>
  <li>It is expected to cover up to VAEs</li>
  <li>The exam is 3 hours long</li>
  <li>No programming questions</li>
</ul>

</div>
        </li>
        
        

    </ul>
</div>
<p>
<h2>Tutorial Schedule</h2>
<div class="home" style="font-size: 0.8em;">

<table style="border-collapse: collapse; width: 100%;">
    <tr style="background-color: white;">
    <td style="border: 1px solid #999; padding: 10px;">Session</td>
    <td style="border: 1px solid #999; padding: 10px;">
      Topics
    </td>
    <td style="border: 1px solid #999; padding: 10px; background-color: white;">Tutor</td>
  </tr>
  <tr style="background-color: #d6f5d6;">
    <td style="border: 1px solid #999; padding: 10px;">Tutorial 1</td>
    <td style="border: 1px solid #999; padding: 10px; font-style: italic;">
      PyTorch Overview -- Tokenization and Embedding
    </td>
    <td style="border: 1px solid #999; padding: 10px; background-color: #d6eaff;">A. Mobasheri</td>
  </tr>
  <tr style="background-color: #d6f5d6;">
    <td style="border: 1px solid #999; padding: 10px;">Tutorial 2</td>
    <td style="border: 1px solid #999; padding: 10px; font-style: italic;">
      Transformers and Large Language Models
    </td>
    <td style="border: 1px solid #999; padding: 10px; background-color: #d6eaff;">A. Mobasheri</td>
  </tr>
  <tr style="background-color: #d6f5d6;">
    <td style="border: 1px solid #999; padding: 10px;">Tutorial 3</td>
    <td style="border: 1px solid #999; padding: 10px; font-style: italic;">
      Auto-regressive Models
    </td>
    <td style="border: 1px solid #999; padding: 10px; background-color: #d6eaff;">M. Safavi</td>
  </tr>
  <tr style="background-color: #d6f5d6;">
    <td style="border: 1px solid #999; padding: 10px;">Tutorial 4</td>
    <td style="border: 1px solid #999; padding: 10px; font-style: italic;">
      Generative Adversarial Networks
    </td>
    <td style="border: 1px solid #999; padding: 10px; background-color: #d6eaff;">A. Mobasheri</td>
  </tr>
  <tr style="background-color: #d6eaff;">
    <td style="border: 1px solid #999; padding: 10px;">Tutorial 5</td>
    <td style="border: 1px solid #999; padding: 10px; font-style: italic;">Exam Overview</td>
    <td style="border: 1px solid #999; padding: 10px;">M. Safavi</td>
  </tr>
  <tr style="background-color: #ffe6cc;">
    <td style="border: 1px solid #999; padding: 10px;"></td>
    <td style="border: 1px solid #999; padding: 10px; color: red; font-weight: bold;">
      Reading Week &amp; Exam - No Lecture
    </td>
    <td style="border: 1px solid #999; padding: 10px; color: red; font-weight: bold;">N/A</td>
  </tr>
  <tr style="background-color: #d6f5d6;">
    <td style="border: 1px solid #999; padding: 10px;">Tutorial 6</td>
    <td style="border: 1px solid #999; padding: 10px; font-style: italic;">
      Variational Inference and VAEs
    </td>
    <td style="border: 1px solid #999; padding: 10px; background-color: #d6eaff;">A. Mobasheri</td>
  </tr>
  <tr style="background-color: #d6f5d6;">
    <td style="border: 1px solid #999; padding: 10px;">Tutorial 7</td>
    <td style="border: 1px solid #999; padding: 10px; font-style: italic;">Diffusion Models I</td>
    <td style="border: 1px solid #999; padding: 10px; background-color: #d6eaff;">M. Safavi</td>
  </tr>
  <tr style="background-color: #d6eaff;">
    <td style="border: 1px solid #999; padding: 10px;">Tutorial 8</td>
    <td style="border: 1px solid #999; padding: 10px; font-style: italic;">Sample Project Demo</td>
    <td style="border: 1px solid #999; padding: 10px;">A. Mobasheri</td>
  </tr>
  <tr style="background-color: #d6f5d6;">
    <td style="border: 1px solid #999; padding: 10px;">Tutorial 9</td>
    <td style="border: 1px solid #999; padding: 10px; font-style: italic;">Diffusion Models II</td>
    <td style="border: 1px solid #999; padding: 10px; background-color: #d6eaff;">M. Safavi</td>
  </tr>
  <tr style="background-color: #f9e6ff;">
    <td style="border: 1px solid #999; padding: 10px;">Tutorial 10</td>
    <td style="border: 1px solid #999; padding: 10px; font-style: italic;">
      Advances and Practical Considerations
    </td>
    <td style="border: 1px solid #999; padding: 10px;">M. Safavi</td>
  </tr>
</table>


</div>
</p>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

<!--     <h2 class="footer-heading">University of Toronto</h2> -->
         <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
 

         <p class="text">
The Edward S. Rogers Sr. Department of Electrical and Computer Engineering<br />
University of Toronto<br />

      </div>

      <div class="footer-col  footer-col-2">
       <ul class="social-media-list">
     

          

          
  <li>
    <a href="https://www.bereyhi.com">
      <i class="fas fa-globe" style="color:gray"></i> bereyhi.com
    </a>
  </li>


          

          

          
  <li>
    <a href="https://www.ece.utoronto.ca">
      <i class="fas fa-globe" style="color:gray"></i> ece.utoronto.ca
    </a>
  </li>




       
        </ul>
      </div>
    </div>

  </div>

</footer>

  </body>

</html>
<!-- d.s.m.s.050600.062508.030515.080516.030818 | "Baby, I'm Yours" -->