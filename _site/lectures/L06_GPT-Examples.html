<p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec3.pdf">Chapter 1 - Section 3</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT-1</a>: Paper <em>Improving Language Understanding by Generative Pre-Training</em> by <em>Alec Radford et al.</em> (OpenAI, 2018) that introduced GPT-1 and revived the idea of pretraining transformers as LMs followed by supervised fine-tuning</li>
  <li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>: Paper <em>Language Models are Unsupervised Multitask Learners</em> by <em>Alec Radford et al.</em> (OpenAI, 2019) that introduces GPT-2 with 1.5B parameter trained on web text</li>
  <li><a href="https://arxiv.org/abs/2005.14165">GPT-3</a>: Paper <em>Language Models are Few-Shot Learners</em> by <em>Tom B. Brown et al.</em> (OpenAI, 2020) that introduces GPT-3, a 175B-parameter transformer LM</li>
  <li>
    <p><a href="https://arxiv.org/abs/2303.08774">GPT-4</a>: <em>GPT-4 Technical Report</em> by <em>OpenAI</em> (2023) that provides an overview of GPT-4’s capabilities</p>
  </li>
  <li><a href="https://arxiv.org/abs/2101.00027">The Pile</a>: Paper <em>The Pile: An 800GB Dataset of Diverse Text for Language Modeling</em> by <em>Leo Gao et al.</em> presented in 2020 introductin dataset <strong>The Pile</strong></li>
  <li><a href="https://arxiv.org/abs/2105.05241">Documentation Debt</a>: Paper <em>Addressing “Documentation Debt” in Machine Learning Research: A Retrospective Datasheet for BookCorpus</em> by <em>Jack Bandy and Nicholas Vincent</em> published in 2021 discussing the efficiency and legality of data collection by looking into <a href="https://arxiv.org/abs/1511.06398">BookCorpus</a></li>
</ul>

