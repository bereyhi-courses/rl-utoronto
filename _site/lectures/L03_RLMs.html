<p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec1.pdf">Chapter 1 - Section 1</a> Pgs 32:42
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/8.pdf">Recurrent LMs</a>: Chapter 8 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li><a href="https://arxiv.org/abs/1708.02182">LSTM LMs</a>: Paper <em>Regularizing and Optimizing LSTM Language Models</em> by <em>Stephen Merity, Nitish Shirish Keskar, and Richard Socher</em> published in ICLR 2018 enabling LSTMs to perform strongly on word-level language modeling</li>
  <li><a href="https://arxiv.org/abs/1711.03953">High-Rank Recurrent LMs</a>: Paper <em>Breaking the Softmax Bottleneck: A High-Rank RNN Language Model</em> by <em>Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen</em> presented at ICLR 2018 proposing Mixture of Softmaxes (MoS) and achieving state-of-the-art results at the time</li>
</ul>
