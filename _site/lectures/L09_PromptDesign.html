<p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec3.pdf">Chapter 1 - Section 3</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought</a>: Paper <em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em> by <em>Jason Wei et al.</em> presented at NeurIPS in 2022 introducing <strong>chain-of-thought</strong> prompting</li>
  <li><a href="https://arxiv.org/abs/2101.00190">Prefix-Tuning</a>: Paper <em>Prefix-Tuning: Optimizing Continuous Prompts for Generation</em> by <em>Xiang Lisa Li et al.</em> presented at ACL in 2021 proposing prefix-tuning approach for prompting</li>
  <li><a href="https://arxiv.org/abs/2104.08691">Prompt-Tuning</a>: Paper <em>The Power of Scale for Parameter-Efficient Prompt Tuning</em> by <em>B. Lester et al.</em> presented at EMNLP in 2021 proposing the prompt tuning idea, i.e., learning to prompt</li>
  <li><a href="https://arxiv.org/abs/2205.11916">Zero-Shot LLMs</a>: Paper <em>Large Language Models are Zero-Shot Reasoners</em> by <em>T. Kojima et al.</em> presented at NeurIPS in 2022 studying zero-shot learning with LLMs</li>
</ul>

