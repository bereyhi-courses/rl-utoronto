<p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec3.pdf">Chapter 1 - Section 3</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://arxiv.org/abs/1511.01432">SSL</a>: Paper <em>Semi-supervised Sequence Learning</em> by <em>Andrew M. Dai et al.</em> published in 2015 that explores using unsupervised pretraining followed by supervised fine-tuning; this was an early solid work advocating <strong>pre-training</strong> idea for LMs</li>
  <li><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT-1</a>: Paper <em>Improving Language Understanding by Generative Pre-Training</em> by <em>Alec Radford et al.</em> (OpenAI, 2018) that introduced GPT-1 and revived the idea of pretraining transformers as LMs followed by supervised fine-tuning</li>
</ul>

