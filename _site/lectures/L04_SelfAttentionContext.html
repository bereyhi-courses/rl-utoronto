<p><strong>Lecture Notes:</strong></p>
<ul>
  <li><a href="/genai-utoronto/assets/Notes/CH1/CH1_Sec2.pdf">Chapter 1 - Section 2</a>
<!-- - [AplDL Notes: Recurent NNs](/genai-utoronto/assets/AplDL/AplDL_RNNs.pdf) --></li>
</ul>

<p><strong>Further Reads:</strong></p>
<ul>
  <li><a href="https://arxiv.org/abs/1706.03762">Transformer Paper</a>: Paper <strong>Attention Is All You Need!</strong> published in 2017 that made a great turn in sequence processing</li>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">Transformers</a>: Chapter 9 of <a href="https://web.stanford.edu/~jurafsky/slp3/">[JM]</a></li>
  <li><a href="https://www.bishopbook.com/">Transformers</a>: Chapter 12 of <a href="https://www.bishopbook.com/">[BB]</a> <strong>Section 12.1</strong></li>
</ul>
