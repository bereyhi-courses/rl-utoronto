<!-- ---
type: lecture
date: 2025-05-06T19:00:00-4:00
title: "Lecture 1: Tokenization and Embedding"
tldr: "LMs - Part 1"
stat: lec
# for lectures stat: lec
description: We start with LMs and understand how we can feed a text into it by doing the so-called "Tokenization" and "Embedding". 
videoID: P7LGgmLU-8g
hide_from_announcments: false
---
**Lecture Notes:**
- [Chapter 1 - Section 1]({{ site.baseurl }}/assets/Notes/CH1/CH1_Sec1.pdf) Pgs 1:18

**Further Reads:**
* [Tokenization](https://web.stanford.edu/~jurafsky/slp3/2.pdf): Chapter 2 of [[JM]](https://web.stanford.edu/~jurafsky/slp3/)
* [Embedding](https://web.stanford.edu/~jurafsky/slp3/6.pdf): Chapter 6 of [[JM]](https://web.stanford.edu/~jurafsky/slp3/)
* [Original BPE Algorithm](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM): Original BPE Algorithm proposed by Philip Gage in 1994
* [BPE for Tokenization](https://arxiv.org/abs/1508.07909): Paper _Neural machine translation of rare words with subword units_ by _Rico Sennrich, Barry Haddow, and Alexandra Birch_ presented in ACL 2016 that adapted BPE for NLP -->